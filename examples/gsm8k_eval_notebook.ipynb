{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e1e13a83",
      "metadata": {
        "id": "e1e13a83"
      },
      "source": [
        "# GSM8K Evaluation Notebook\n",
        "\n",
        "This notebook evaluates Gemma JAX on the GSM8K mathematical reasoning dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "578b4ead",
      "metadata": {
        "id": "578b4ead"
      },
      "source": [
        "### Imports and other boilerplate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_pfZeXU0fstZ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pfZeXU0fstZ",
        "outputId": "83bebad0-9552-4c14-d00b-e7f341e194c7"
      },
      "outputs": [],
      "source": [
        "# Uncomment if using Colab\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os # Git clone the repository if it does not exist, and cd into it.\n",
        "os.chdir('/content/drive/My Drive')\n",
        "\n",
        "if not os.path.exists('gemma-jax'):\n",
        "  !git clone https://github.com/baricev/gemma-jax\n",
        "\n",
        "os.makedirs('gemma-jax', exist_ok=True)\n",
        "os.chdir('gemma-jax')\n",
        "\n",
        "print(f\"Current working directory: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HkRWXGphGhXK",
      "metadata": {
        "id": "HkRWXGphGhXK"
      },
      "source": [
        "### Install\n",
        "\n",
        "`!pip install jax[tpu] orbax datasets` -- quiet should also work"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AjuCOVz0f9wj",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AjuCOVz0f9wj",
        "outputId": "80c71df5-7d0d-4256-dda5-167c39470a61"
      },
      "outputs": [],
      "source": [
        "! pip install -e . --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-5zRKOPqG8L0",
      "metadata": {
        "id": "-5zRKOPqG8L0"
      },
      "source": [
        "### Package Imports\n",
        "\n",
        "Import core gemma-jax functions and datastructures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5d48b8f",
      "metadata": {
        "id": "c5d48b8f"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import argparse\n",
        "from functools import partial\n",
        "from pathlib import Path\n",
        "import jax\n",
        "from jax import Array\n",
        "import jax.numpy as jnp\n",
        "import re\n",
        "import datasets\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Assuming gemma_jax is installed in editable mode (`pip install -e .`)\n",
        "from gemma_jax.core.weights import (\n",
        "    create_gemma3_config,\n",
        "    create_device_mesh,\n",
        "    load_model\n",
        ")\n",
        "from gemma_jax.core.model import (\n",
        "    forward_fn,\n",
        "    setup_scan_fn,\n",
        "    scan_generate_step\n",
        ")\n",
        "from gemma_jax.core.cache import (\n",
        "  KVCache,\n",
        "  LayoutType,\n",
        "  init_cache,\n",
        "  aliases_map,\n",
        "  layout_map,\n",
        "  shard_dims,\n",
        "  SEQUENCE_HEADS,\n",
        "  HEADS_SEQUENCE,\n",
        ")\n",
        "from gemma_jax.core.rope import load_rope_cache\n",
        "from gemma_jax.core.sp_tokenizer import SentencePieceTokenizer, process_and_pad_inputs, encode_text, decode_tokens, format_prompt\n",
        "from gemma_jax.core.inference import greedy_sample"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a203cec",
      "metadata": {
        "id": "1a203cec"
      },
      "source": [
        "## Configuration Defaults\n",
        "\n",
        "Set the default configuration values here. These replace the command-line arguments used in the script version.\n",
        "\n",
        "**Important:** Update `CHECKPOINT_PATH` and `TOKENIZER_PATH` to your actual absolute paths.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ndFWl6LpS15H",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndFWl6LpS15H",
        "outputId": "72e17804-ca12-4fee-f6e1-808732dbf6ee"
      },
      "outputs": [],
      "source": [
        "root_dir = Path.cwd() # Assuming this notebook is in the `gemma_jax/examples` directory\n",
        "checkpoint_path = root_dir / \"4b\"               # TODO: Replace with your ABSOLUTE path\n",
        "tokenizer_path = root_dir / \"tokenizer.model\"   # TODO: Replace with your ABSOLUTE path\n",
        "\n",
        "try:\n",
        "  assert tokenizer_path.exists(), f\"Tokenizer path {tokenizer_path} does not exist.\"\n",
        "except AssertionError:\n",
        "  # If the tokenizer path is not set, assume we are running a notebook in the examples directory\n",
        "  root_dir = Path(__file__).parent.parent         # Adjust this if the notebook is in a different directory\n",
        "  tokenizer_path = root_dir / \"tokenizer.model\"\n",
        "  checkpoint_path= root_dir / \"4b\"              # TODO: Replace with your ABSOLUTE path\n",
        "\n",
        "print(f\"Using default tokenizer path: {tokenizer_path}\")\n",
        "print(f\"Using default checkpoint path: {checkpoint_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Dp4ylz2iTZmI",
      "metadata": {
        "id": "Dp4ylz2iTZmI"
      },
      "source": [
        "### Model Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58df04fa",
      "metadata": {
        "id": "58df04fa"
      },
      "outputs": [],
      "source": [
        "model_size = 4              # Gemma model size (e.g., 4 for 4B). Choices: [1, 4, 12, 27]\n",
        "cache_length = 1024 * 2     # KV cache length.\n",
        "padded_input_size = 2048    # Padded input sequence length.\n",
        "window_size = 1024          # Attention window size for sliding window attention.\n",
        "batch_size = 4              # Batch size for inference.\n",
        "generate_steps = 1024          # Number of tokens to generate after prefill.\n",
        "dtype_str = \"bfloat16\"       # Data type for model parameters. Choices: ['bfloat16', 'float16', 'float32']\n",
        "\n",
        "dtype_map = {\n",
        "    \"bfloat16\": jnp.bfloat16,\n",
        "    \"float16\": jnp.float16,\n",
        "    \"float32\": jnp.float32,\n",
        "}\n",
        "model_dtype = dtype_map[dtype_str]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de352b8b",
      "metadata": {
        "id": "de352b8b"
      },
      "source": [
        "## Setup: Initialization\n",
        "\n",
        "The cell below initializes the tokenizer, model configuration, device mesh, loads the model parameters, and initializes the KV and RoPE caches.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3777e2e3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3777e2e3",
        "outputId": "82f61b99-b7a8-4e70-bbd9-c6e5c71c39c7"
      },
      "outputs": [],
      "source": [
        "print(\"Starting setup...\")\n",
        "start_setup = time.time()\n",
        "\n",
        "# 1. Model Config\n",
        "config = create_gemma3_config(\n",
        "    model_size=model_size,\n",
        "    batch_size=batch_size,\n",
        "    padded_input_size=padded_input_size,\n",
        "    cache_length=cache_length,\n",
        "    window_size=window_size,\n",
        "    generate_steps = generate_steps,\n",
        ")\n",
        "print(f\"Model Config created for Gemma-{model_size}b\")\n",
        "\n",
        "# 2. Device Mesh\n",
        "num_devices = len(jax.devices())\n",
        "# TODO: Configure mesh shape\n",
        "mesh = create_device_mesh()\n",
        "print(f\"Device mesh created with shape: {mesh.shape}\")\n",
        "\n",
        "# 3. Load Model\n",
        "assert checkpoint_path.exists(), f\"Checkpoint path {checkpoint_path} does not exist.\"\n",
        "assert checkpoint_path.is_absolute(), f\"Checkpoint path {checkpoint_path} must be an absolute path.\"\n",
        "\n",
        "print(f\"Loading model from: {checkpoint_path} (dtype: {dtype_str})...\")\n",
        "load_start = time.time()\n",
        "model = load_model(checkpoint_path, mesh, config, dtype=model_dtype)\n",
        "print(f\"Model loaded in {time.time() - load_start:.2f}s\")\n",
        "\n",
        "# 4. Initialize Caches\n",
        "# rope_cache = load_rope_cache(mesh, config)  # RoPE cache dtype is float32 internally\n",
        "rope_cache = None  # pass None to compute embeddings at runtime\n",
        "\n",
        "# Configure memory layout, sharding or chache update functions in \"cache.py\"\n",
        "# or use pre-configured settings (SEQUENCE_HEADS, HEADS_SEQUENCE)\n",
        "cache_layout =  SEQUENCE_HEADS\n",
        "\n",
        "cache = init_cache(\n",
        "    mesh=mesh,\n",
        "    config=config,\n",
        "    dtype=jnp.bfloat16,\n",
        "    kind=cache_layout,\n",
        "    layout_map=layout_map,\n",
        ")\n",
        "\n",
        "print(f\"Setup complete in {time.time() - start_setup:.2f}s\")\n",
        "\n",
        "# 4. Create Tokenizer\n",
        "assert tokenizer_path.exists(), f\"Tokenizer path {tokenizer_path} does not exist.\"\n",
        "assert tokenizer_path.is_absolute(), f\"Tokenizer path {tokenizer_path} must be an absolute path.\"\n",
        "tokenizer = SentencePieceTokenizer(tokenizer_path)\n",
        "print(f\"Tokenizer loaded from: {tokenizer_path}\")\n",
        "\n",
        "print(f\"Setup complete in {time.time() - start_setup:.2f}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61b54cb9",
      "metadata": {
        "id": "61b54cb9"
      },
      "source": [
        "## Evaluation Constants\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50425319",
      "metadata": {
        "id": "50425319"
      },
      "outputs": [],
      "source": [
        "# --- Role IDs ---\n",
        "ROLE_USER, ROLE_MODEL, ROLE_SYSTEM = 0, 1, 2\n",
        "ROLE_STR = {ROLE_USER: \"user\", ROLE_MODEL: \"model\", ROLE_SYSTEM: \"system\"}\n",
        "\n",
        "# --- Tokenizer Constants ---\n",
        "PAD_ID: int = 0\n",
        "EOS_ID: int = 1\n",
        "BOS_ID: int = 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b01e2b05",
      "metadata": {
        "id": "b01e2b05"
      },
      "source": [
        "## Prompt Templates\n",
        "\n",
        "GSM8K prompts taken from:\n",
        "\n",
        "[google-deepmind/gemma/tree/main/colabs/old/gsm8k_eval.ipynb](https://github.com/google-deepmind/gemma/blob/2a162e21be390aa0ec635deb7176fb64fb1868b1/colabs/old/gsm8k_eval.ipynb)\n",
        "\n",
        "See commit: 2a162e21be390aa0ec635deb7176fb64fb1868b1 for original work.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a518dad9",
      "metadata": {
        "id": "a518dad9"
      },
      "outputs": [],
      "source": [
        "from gemma_jax.core.gsm8k_eval import PREAMBLE, EXTRA_3_SHOTS, FEWSHOT , PROMPT as EIGHT_SHOT_PROMPT\n",
        "\n",
        "# --- System prompt ---\n",
        "PREAMBLE = \"\"\"As an expert problem solver solve step by step the following mathematical questions.\"\"\"\n",
        "\n",
        "FEWSHOT = \"\"\"\n",
        "Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\n",
        "A: We start with 15 trees. Later we have 21 trees. The difference must be the number of trees they planted. So, they must have planted 21 - 15 = 6 trees. The answer is 6.\n",
        "\n",
        "Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
        "A: There are 3 cars in the parking lot already. 2 more arrive. Now there are 3 + 2 = 5 cars. The answer is 5.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9448f3d",
      "metadata": {
        "id": "d9448f3d"
      },
      "source": [
        "## Evaluation Setup\n",
        "\n",
        "Creat a text processing function and inference function. These are passed to the benchmarking function to run the evaluation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mBzdOD-Nf82z",
      "metadata": {
        "id": "mBzdOD-Nf82z"
      },
      "outputs": [],
      "source": [
        "process_partial = partial(\n",
        "    process_and_pad_inputs,\n",
        "    max_sequence_length=padded_input_size,\n",
        "    cache_len=cache_length,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "D2bEKcGXcTmw",
      "metadata": {
        "id": "D2bEKcGXcTmw"
      },
      "source": [
        "### Prefill and Generate Functions\n",
        "\n",
        "The `prefill_partial` function is used to prefill the model with the input tokens. It takes the padded input IDs, positions, and attention mask as input and returns the logits and updated cache. Note: The cache object is updated in-place by prefill_partial\n",
        "\n",
        "Setup the scan function with the model, cache, and other parameters using `setup_scan_fn`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84wiiqk6cTDE",
      "metadata": {
        "id": "84wiiqk6cTDE"
      },
      "outputs": [],
      "source": [
        "# Prefill inputs\n",
        "prefill_partial = partial(\n",
        "    forward_fn,\n",
        "    write_index=0,\n",
        "    model=model,\n",
        "    cache=cache,\n",
        "    rope_cache=rope_cache,\n",
        "    config=config,\n",
        "    layout=cache_layout,\n",
        ")\n",
        "\n",
        "# Auto-regressive generation\n",
        "generate_partial = partial(\n",
        "    scan_generate_step,\n",
        "    model=model,\n",
        "    rope_cache=rope_cache,\n",
        "    config=config,\n",
        "    layout=cache_layout,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7839419a",
      "metadata": {
        "id": "7839419a"
      },
      "source": [
        "## Helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e0e7948",
      "metadata": {
        "id": "4e0e7948"
      },
      "outputs": [],
      "source": [
        "def find_numbers(x: str) -> list[str]:\n",
        "    return re.compile(r\"-?[\\d,]*\\.?\\d+\").findall(x)\n",
        "\n",
        "def find_number(x: str, answer_delimiter: str = \"The answer is\") -> str:\n",
        "    if answer_delimiter in x:\n",
        "        answer = x.split(answer_delimiter)[-1]\n",
        "        numbers = find_numbers(answer)\n",
        "        if numbers:\n",
        "            return numbers[0]\n",
        "    numbers = find_numbers(x)\n",
        "    if numbers:\n",
        "        return numbers[-1]\n",
        "    return \"\"\n",
        "\n",
        "def normalize_answer(ans: str) -> str:\n",
        "    ans = ans.strip().replace(\",\", \"\")\n",
        "    ans = re.sub(r\"[^\\d\\.\\-]+$\", \"\", ans)\n",
        "    return ans\n",
        "\n",
        "# --- Utilities ----\n",
        "\n",
        "_NUM_RE = r\"-?[\\d,]*\\.?\\d+\"\n",
        "\n",
        "\n",
        "def _strip(s: str) -> str:\n",
        "    return s.strip(\" \\n\\t\")\n",
        "\n",
        "\n",
        "def _visible_segment(raw: str) -> str:\n",
        "    \"\"\"\n",
        "    Pull out the *model-visible* text from the raw dump emitted by\n",
        "    `chat_once_batched`.  Supports both:\n",
        "         “<start_of_turn>model … <end_of_turn>”\n",
        "         “user\\n … model\\n …”\n",
        "    \"\"\"\n",
        "    # 1) Try the explicit tag format first\n",
        "    m = re.search(r\"<start_of_turn>model(.*?)<end_of_turn>\", raw, flags=re.S)\n",
        "    if m:\n",
        "        return _strip(m.group(1))\n",
        "\n",
        "    # 2) Fallback: last “…\\nmodel\\n” chunk\n",
        "    if \"\\nmodel\" in raw:\n",
        "        # Split by the LAST occurrence to avoid grabbing few-shot exemplars\n",
        "        head, _, tail = raw.rpartition(\"\\nmodel\")\n",
        "        return _strip(tail)\n",
        "\n",
        "    # 3) Nothing matched -return full string (best effort)\n",
        "    return _strip(raw)\n",
        "\n",
        "\n",
        "def _last_answer_block(visible: str) -> str:\n",
        "    \"\"\"\n",
        "    Given the visible text, grab the **final** answer paragraph:\n",
        "\n",
        "        … Q: <few-shot #N>\n",
        "           A: <few-shot #N answer>\n",
        "        Q: <real question>\n",
        "        A: <real answer>\n",
        "\n",
        "    We split on *lines* that start with 'Q:' or 'A:' and keep the\n",
        "    trailing answer.\n",
        "    \"\"\"\n",
        "    # Normalise line endings & get rid of leading markdown bullets\n",
        "    lines = [l.lstrip(\"* \").rstrip() for l in visible.replace(\"\\r\", \"\").split(\"\\n\")]\n",
        "\n",
        "    # Walk backwards to find the last line that *begins* with 'A:'\n",
        "    for i in range(len(lines) - 1, -1, -1):\n",
        "        if lines[i].startswith((\"A:\", \"Answer:\", \"**Answer:**\")):\n",
        "            # Everything FROM that line onwards is the answer block\n",
        "            return \"\\n\".join(lines[i:])\n",
        "\n",
        "    # Fallback -no marker, keep entire visible\n",
        "    return visible\n",
        "\n",
        "\n",
        "def _extract_number(ans_block: str) -> str:\n",
        "    \"\"\"\n",
        "    Pull the numeric answer out of the final answer block.\n",
        "    Priority:\n",
        "        1.  “The answer is <num>”\n",
        "        2.  “Answer: <blah> <num>”\n",
        "        3.  Last number in the block\n",
        "    TODO:\n",
        "    handle model outputs with \"commas\" or other variations, for example, 'model_answer': 99,076.92 ,   'predicted': '99076.92', is a valid answer, but missed by the parser.\n",
        "\n",
        "    \"\"\"\n",
        "    # 1) “The answer is<num>”\n",
        "    m = re.search(r\"The answer is\\s*(%s)\" % _NUM_RE, ans_block, flags=re.I)\n",
        "    if m:\n",
        "        return _strip(m.group(1)).replace(\",\", \"\")\n",
        "\n",
        "    # 2) “Answer:” / “**Answer:**”\n",
        "    m = re.search(r\"Answer[:\\s\\*]*.*?(%s)\" % _NUM_RE, ans_block, flags=re.I | re.S)\n",
        "    if m:\n",
        "        return _strip(m.group(1)).replace(\",\", \"\")\n",
        "\n",
        "    # 3) Anything else -last number in the block\n",
        "    nums = re.findall(_NUM_RE, ans_block)\n",
        "    return nums[-1].replace(\",\", \"\") if nums else \"\"\n",
        "\n",
        "\n",
        "def _parse_reply(raw_reply: str) -> tuple[str, str]:\n",
        "    \"\"\"\n",
        "    Returns (visible_str, numeric_prediction)\n",
        "    \"\"\"\n",
        "    visible = _visible_segment(raw_reply)\n",
        "    answer_block = _last_answer_block(visible)\n",
        "    num = _extract_number(answer_block)\n",
        "    return visible, num\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6519013f",
      "metadata": {
        "id": "6519013f"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "Key functions from `chat.py` are `create_empty_state_batched` (used to initialize stateful object and `chat`, which orchestrates the inference loop.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3WoOyCLifDYt",
      "metadata": {
        "id": "3WoOyCLifDYt"
      },
      "outputs": [],
      "source": [
        "from gemma_jax.core.conversation_state import create_empty_state_batched, chat\n",
        "\n",
        "QUESTION_TEMPLATE = \"\\nQ: {question}\\nA:\"\n",
        "\n",
        "def evaluate_gsm8k_batched(\n",
        "    batch_size: int = 4,\n",
        "    save_path: str = \"report.json\",\n",
        "    generate_steps: int = 256,\n",
        "    verbose: bool = True,\n",
        "    save_every: int = 50,\n",
        "    print_every: int = 20,\n",
        "    max_num_examples: int | None = None,\n",
        "    max_turns: int = 10,\n",
        "    tokenizer=tokenizer,\n",
        "    config=config,\n",
        "    benchmark=FEWSHOT,\n",
        "):\n",
        "\n",
        "    #  0.  Load data & assemble prompts\n",
        "    gsm8k = datasets.load_dataset(\"gsm8k\", \"main\")[\"test\"]\n",
        "    if max_num_examples is not None:\n",
        "        gsm8k = gsm8k.select(range(max_num_examples))\n",
        "\n",
        "\n",
        "    prompts = [\n",
        "        format_prompt(\n",
        "            f\"{PREAMBLE}\\n{FEWSHOT}{QUESTION_TEMPLATE.format(question=ex['question'])}\"\n",
        "        )\n",
        "        for ex in gsm8k\n",
        "    ]\n",
        "    ground_truths = [ex[\"answer\"] for ex in gsm8k]\n",
        "    questions = [ex[\"question\"] for ex in gsm8k]\n",
        "    num_examples = len(prompts)\n",
        "    print(f\"Number of examples: {num_examples}\")\n",
        "\n",
        "    #  1.  Initialise chat state\n",
        "    conv_state = create_empty_state_batched(\n",
        "        batch_size=config.batch_size,\n",
        "        cache_length= config.cache_length,\n",
        "        max_turns=max_turns,\n",
        "        with_trace=False,\n",
        "        pad_id=PAD_ID,\n",
        "    )\n",
        "\n",
        "    # 2. Set up the initial conversation state\n",
        "    chat_partial = partial(chat,\n",
        "        prefill_partial,\n",
        "        process_partial,\n",
        "        setup_scan_fn,\n",
        "        generate_partial,\n",
        "        tokenizer,\n",
        "        config,\n",
        "    )\n",
        "\n",
        "\n",
        "    # Book-keeping accumulators\n",
        "    results: list[dict] = []\n",
        "    correct = 0\n",
        "    truncations = 0\n",
        "\n",
        "    print(f\"Evaluating{num_examples} examples \" f\"in batches of{batch_size} …\")\n",
        "\n",
        "    #  3.  Main loop -batched inference + parsing\n",
        "    for i in tqdm(range(0, num_examples, batch_size), desc=\"GSM8K batched\"):\n",
        "\n",
        "        batch_prompts = prompts[i : i + batch_size]\n",
        "\n",
        "        # Guard: last batch may be smaller than `batch_size`\n",
        "        cur_bs = len(batch_prompts)\n",
        "        if cur_bs < batch_size:\n",
        "            # Skip if not multiple of batch size\n",
        "            continue\n",
        "        else:\n",
        "            conv_state_batch = conv_state\n",
        "\n",
        "        _, batch_replies = chat_partial(\n",
        "            conv_state_batch,\n",
        "            batch_prompts,\n",
        "            generate_steps=generate_steps,\n",
        "            role=ROLE_USER,\n",
        "        )\n",
        "\n",
        "        # Vectorised parsing (no JIT -but consistent shape for later)\n",
        "        batch_vis_preds = list(map(_parse_reply, batch_replies))\n",
        "\n",
        "        #  4.  Per-example metrics & logging\n",
        "        for j, ((visible, pred), question, gt_raw) in enumerate(\n",
        "            zip(\n",
        "                batch_vis_preds,\n",
        "                questions[i : i + cur_bs],\n",
        "                ground_truths[i : i + cur_bs],\n",
        "            )\n",
        "        ):\n",
        "            idx = i + j\n",
        "            truth = _extract_number(gt_raw)\n",
        "            is_correct = pred == truth\n",
        "\n",
        "            # Simple truncation heuristic -no answer block found\n",
        "            is_truncated = pred == \"\"\n",
        "\n",
        "            if is_correct:\n",
        "                correct += 1\n",
        "            if is_truncated:\n",
        "                truncations += 1\n",
        "\n",
        "            results.append(\n",
        "                {\n",
        "                    \"idx\": idx,\n",
        "                    \"question\": question,\n",
        "                    \"ground_truth\": truth,\n",
        "                    \"model_answer\": visible,\n",
        "                    \"predicted\": pred,\n",
        "                    \"is_correct\": is_correct,\n",
        "                    \"truncated\": is_truncated,\n",
        "                    \"raw\": batch_replies[j],\n",
        "                }\n",
        "            )\n",
        "\n",
        "            # TODO: print all incorrect answers by addin: or not is_correct):\n",
        "            if verbose and ((idx + 1) % print_every == 0):\n",
        "                running_acc = correct / (idx + 1)\n",
        "                running_trunc = truncations / (idx + 1)\n",
        "                print(\n",
        "                    f\"\\n--- Example{idx+1}/{num_examples}\"\n",
        "                    f\"\\nQ: {question}\"\n",
        "                    f\"\\nGT: {truth}\\nPred: {pred} | Correct: {is_correct}\"\n",
        "                    f\"\\nRunning Acc: {running_acc:.2%} |Trunc rate: {running_trunc:.2%}\"\n",
        "                )\n",
        "\n",
        "            # periodic checkpoint\n",
        "            if (idx + 1) % save_every == 0:\n",
        "                with open(save_path, \"w\") as fp:\n",
        "                    json.dump(results, fp, indent=2)\n",
        "                if verbose:\n",
        "                    print(f\"[checkpoint] saved{idx+1} examples → {save_path}\")\n",
        "\n",
        "    #  5.  Final stats + save\n",
        "    accuracy = correct / len(results) if len(results) > 0 else 0\n",
        "    print(\n",
        "        f\"\\nGSM8K accuracy: {accuracy:.2%} \"\n",
        "        f\"({correct}/{len(results)})  |truncations: {truncations}\"\n",
        "    )\n",
        "\n",
        "    with open(save_path, \"w\") as fp:\n",
        "        json.dump(results, fp, indent=2)\n",
        "    print(f\"[final] wrote results to {save_path}\")\n",
        "\n",
        "    return accuracy, results\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "273153b9",
      "metadata": {
        "id": "273153b9"
      },
      "source": [
        "## Results\n",
        "\n",
        "### Execute the cell below to run the full evaluation\n",
        "\n",
        "Note: This may take significant time depending on hardwar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90157cf5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90157cf5",
        "outputId": "1903bdf1-9fb8-45ae-e6db-f12efe33fd7d"
      },
      "outputs": [],
      "source": [
        "### Execute this cell to run the full evaluation\n",
        "# Note: This may take significant time depending on hardware\n",
        "\n",
        "accuracy, results = evaluate_gsm8k_batched(\n",
        "    batch_size=1,\n",
        "    generate_steps=1024,\n",
        "    save_every=100,\n",
        "    print_every=40,\n",
        "    verbose=True,\n",
        "    max_num_examples=120\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MePKp7bq1v00",
      "metadata": {
        "id": "MePKp7bq1v00"
      },
      "source": [
        "## Gemma 3 Technical Report\n",
        "\n",
        "\n",
        "The Gemma 3 technical report  (https://arxiv.org/pdf/2503.19786v1) provides the following results for the GSM8K benchmark:\n",
        "\n",
        "\n",
        "- for the instruction fine-tuned 4B model (the one used in this notebook) they report a score of 89.2% (8-shot, CoT).\n",
        "- The score for the pre-trained model was 38.4%.\n",
        "\n",
        "Our results: \n",
        "- 61.67% , on 120 examples, using the FEWSHOT prompt, and with no CoT.\n",
        "\n",
        "See Table 10, and Table 18 in the paper for full results."
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V6E1",
      "machine_shape": "hm",
      "provenance": []
    },
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
