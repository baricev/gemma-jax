{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a91c78f7",
      "metadata": {
        "id": "a91c78f7"
      },
      "source": [
        "## Gemma JAX Multi-Turn Chat Notebook\n",
        "\n",
        "This notebook demonstrates running batched, multi-turn inference with the Gemma JAX model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "578b4ead",
      "metadata": {
        "id": "578b4ead"
      },
      "source": [
        "### Imports and other boilerplate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_pfZeXU0fstZ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pfZeXU0fstZ",
        "outputId": "8e25e6b4-0e83-4c17-9b74-ec6d1417373f"
      },
      "outputs": [],
      "source": [
        "# Uncomment if using Colab\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os # Git clone the repository if it does not exist, and cd into it.\n",
        "os.chdir('/content/drive/My Drive')\n",
        "\n",
        "if not os.path.exists('gemma-jax'):\n",
        "  !git clone https://github.com/baricev/gemma-jax\n",
        "\n",
        "os.makedirs('gemma-jax', exist_ok=True)\n",
        "os.chdir('gemma-jax')\n",
        "\n",
        "print(f\"Current working directory: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HkRWXGphGhXK",
      "metadata": {
        "id": "HkRWXGphGhXK"
      },
      "source": [
        "### Install\n",
        "\n",
        "`!pip install jax[tpu] orbax datasets` -- quiet should also work"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AjuCOVz0f9wj",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AjuCOVz0f9wj",
        "outputId": "4cafc7c0-3830-4b25-fa5b-98a6ca2cd921"
      },
      "outputs": [],
      "source": [
        "!pip install -e . --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-5zRKOPqG8L0",
      "metadata": {
        "id": "-5zRKOPqG8L0"
      },
      "source": [
        "### Package Imports\n",
        "\n",
        "Import core gemma-jax functions and datastructures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "c5d48b8f",
      "metadata": {
        "id": "c5d48b8f"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import argparse\n",
        "from functools import partial\n",
        "from pathlib import Path\n",
        "import jax\n",
        "from jax import Array\n",
        "import jax.numpy as jnp\n",
        "\n",
        "# Assuming gemma_jax is installed in editable mode (`pip install -e .`)\n",
        "from gemma_jax.core.weights import (\n",
        "    create_gemma3_config,\n",
        "    create_device_mesh,\n",
        "    load_model\n",
        ")\n",
        "from gemma_jax.core.model import (\n",
        "    forward_fn,\n",
        "    setup_scan_fn,\n",
        "    scan_generate_step\n",
        ")\n",
        "from gemma_jax.core.cache import (\n",
        "  KVCache,\n",
        "  LayoutType,\n",
        "  SEQUENCE_HEADS,\n",
        "  HEADS_SEQUENCE,\n",
        "  init_cache,\n",
        "  layout_map,\n",
        ")\n",
        "from gemma_jax.core.rope import load_rope_cache\n",
        "from gemma_jax.core.sp_tokenizer import SentencePieceTokenizer, process_and_pad_inputs, encode_text, decode_tokens, format_prompt\n",
        "from gemma_jax.core.inference import greedy_sample"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a203cec",
      "metadata": {
        "id": "1a203cec"
      },
      "source": [
        "## Configuration Defaults\n",
        "\n",
        "Set the default configuration values here. These replace the command-line arguments used in the script version.\n",
        "\n",
        "**Important:** Update `CHECKPOINT_PATH` and `TOKENIZER_PATH` to your actual absolute paths.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ndFWl6LpS15H",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndFWl6LpS15H",
        "outputId": "7567158c-bf5c-44c4-c191-e9a1a2f1d5bb"
      },
      "outputs": [],
      "source": [
        "root_dir = Path.cwd() # Assuming this notebook is in the `gemma_jax/examples` directory\n",
        "checkpoint_path = root_dir / \"4b\"               # TODO: Replace with your ABSOLUTE path\n",
        "tokenizer_path = root_dir / \"tokenizer.model\"   # TODO: Replace with your ABSOLUTE path\n",
        "\n",
        "try:\n",
        "  assert tokenizer_path.exists(), f\"Tokenizer path {tokenizer_path} does not exist.\"\n",
        "except AssertionError:\n",
        "  # If the tokenizer path is not set, assume we are running a notebook in the examples directory\n",
        "  root_dir = Path(__file__).parent.parent         # Adjust this if the notebook is in a different directory\n",
        "  tokenizer_path = root_dir / \"tokenizer.model\"\n",
        "  checkpoint_path= root_dir / \"4b\"              # TODO: Replace with your ABSOLUTE path\n",
        "\n",
        "print(f\"Using default tokenizer path: {tokenizer_path}\")\n",
        "print(f\"Using default checkpoint path: {checkpoint_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Dp4ylz2iTZmI",
      "metadata": {
        "id": "Dp4ylz2iTZmI"
      },
      "source": [
        "### Model Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "58df04fa",
      "metadata": {
        "id": "58df04fa"
      },
      "outputs": [],
      "source": [
        "model_size = 4              # Gemma model size (e.g., 4 for 4B). Choices: [1, 4, 12, 27]\n",
        "cache_length = 2048         # KV cache length.\n",
        "padded_input_size = 1024    # Padded input sequence length.\n",
        "window_size = 1024          # Attention window size for sliding window attention.\n",
        "batch_size = 4              # Batch size for inference.\n",
        "generate_steps = 4          # Number of tokens to generate after prefill.\n",
        "dtype_str = \"bfloat16\"       # Data type for model parameters. Choices: ['bfloat16', 'float16', 'float32']\n",
        "\n",
        "dtype_map = {\n",
        "    \"bfloat16\": jnp.bfloat16,\n",
        "    \"float16\": jnp.float16,\n",
        "    \"float32\": jnp.float32,\n",
        "}\n",
        "model_dtype = dtype_map[dtype_str]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de352b8b",
      "metadata": {
        "id": "de352b8b"
      },
      "source": [
        "## Setup: Initialization\n",
        "\n",
        "The cell below initializes the tokenizer, model configuration, device mesh, loads the model parameters, and initializes the KV and RoPE caches.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3777e2e3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3777e2e3",
        "outputId": "d32ba75c-4455-41a2-a9b3-7daeab217743"
      },
      "outputs": [],
      "source": [
        "print(\"Starting setup...\")\n",
        "start_setup = time.time()\n",
        "\n",
        "# 1. Model Config\n",
        "config = create_gemma3_config(\n",
        "    model_size=model_size,\n",
        "    batch_size=batch_size,\n",
        "    padded_input_size=padded_input_size,\n",
        "    cache_length=cache_length,\n",
        "    window_size=window_size,\n",
        ")\n",
        "print(f\"Model Config created for Gemma-{model_size}b\")\n",
        "\n",
        "# 2. Device Mesh\n",
        "num_devices = len(jax.devices())\n",
        "# TODO: Configure mesh shape\n",
        "mesh = create_device_mesh((2, num_devices//2))\n",
        "print(f\"Device mesh created with shape: {mesh.shape}\")\n",
        "\n",
        "\n",
        "# 3. Load Model\n",
        "assert checkpoint_path.exists(), f\"Checkpoint path {checkpoint_path} does not exist.\"\n",
        "assert checkpoint_path.is_absolute(), f\"Checkpoint path {checkpoint_path} must be an absolute path.\"\n",
        "\n",
        "print(f\"Loading model from: {checkpoint_path} (dtype: {dtype_str})...\")\n",
        "load_start = time.time()\n",
        "model = load_model(checkpoint_path, mesh, config, dtype=model_dtype)\n",
        "print(f\"Model loaded in {time.time() - load_start:.2f}s\")\n",
        "\n",
        "# 4. Initialize Caches\n",
        "# rope_cache = load_rope_cache(mesh, config)  # RoPE cache dtype is float32 internally\n",
        "rope_cache = None  # pass None to compute embeddings at runtime\n",
        "\n",
        "# Configure memory layout, sharding or chache update functions in \"cache.py\"\n",
        "# or use pre-configured settings (SEQUENCE_HEADS, HEADS_SEQUENCE)\n",
        "cache_layout =  SEQUENCE_HEADS\n",
        "\n",
        "cache = init_cache(\n",
        "    mesh=mesh,\n",
        "    config=config,\n",
        "    dtype=jnp.bfloat16,\n",
        "    kind=cache_layout,\n",
        "    layout_map=layout_map,\n",
        ")\n",
        "\n",
        "print(f\"Setup complete in {time.time() - start_setup:.2f}s\")\n",
        "\n",
        "# 4. Create Tokenizer\n",
        "assert tokenizer_path.exists(), f\"Tokenizer path {tokenizer_path} does not exist.\"\n",
        "assert tokenizer_path.is_absolute(), f\"Tokenizer path {tokenizer_path} must be an absolute path.\"\n",
        "tokenizer = SentencePieceTokenizer(tokenizer_path)\n",
        "print(f\"Tokenizer loaded from: {tokenizer_path}\")\n",
        "\n",
        "print(f\"Setup complete in {time.time() - start_setup:.2f}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WmmFbkZYqHT3",
      "metadata": {
        "id": "WmmFbkZYqHT3"
      },
      "source": [
        "## Input Processing, Prefill and Generate Functions\n",
        "\n",
        "Tokenize and encode the input text. The tokenizer is a SentencePiece wrapper. Setup the prefill and auto-regressive stages using the functions defined above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf886420",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bf886420",
        "outputId": "f28e44a3-441a-43b2-da2d-512cd47dbf7a"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Process inputs\n",
        "process_partial = partial(\n",
        "    process_and_pad_inputs,\n",
        "    max_sequence_length=padded_input_size,\n",
        "    cache_len=cache_length,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "# Prefill inputs\n",
        "prefill_partial = partial(\n",
        "    forward_fn,\n",
        "    write_index=0,\n",
        "    model=model,\n",
        "    cache=cache,\n",
        "    rope_cache=rope_cache,\n",
        "    config=config,\n",
        "    layout=cache_layout,\n",
        ")\n",
        "\n",
        "# Auto-regressive generation\n",
        "generate_partial = partial(\n",
        "    scan_generate_step,\n",
        "    model=model,\n",
        "    rope_cache=rope_cache,\n",
        "    config=config,\n",
        "    layout=cache_layout,\n",
        ")\n",
        "\n",
        "\n",
        "# Example input text\n",
        "input_text = [\n",
        "    \"I love to\",\n",
        "    \"Explain general relativity to a first-century Roman philosopher (Cicero)\",\n",
        "    \"Explain Cantor's proof of the uncountability of the reals to a Babylonian mathematician\",\n",
        "    \"Why is the sky blue?\",\n",
        "]\n",
        "\n",
        "\n",
        "prompt = [\"Explaing evolution to a first-century Roman philosopher (Cicero)\"]\n",
        "prompt += [\"Explain the significance of the following quote `The only thing we have to fear is fear itself`\"]\n",
        "prompt += [\"I love to\"] * batch_size  # Repeat the input text for the batch size\n",
        "input_text = prompt[:batch_size]  # Ensure the input text matches the batch size\n",
        "\n",
        "input_text = [format_prompt(text) for text in input_text]  # Format for Gemma 3 dialogue\n",
        "ids = encode_text(input_text, tokenizer, add_bos_token_only=True)\n",
        "input_text, ids.shape, ids\n",
        "\n",
        "# Process and pad inputs\n",
        "raw_input_ids = encode_text(input_text, tokenizer)\n",
        "attn_mask = raw_input_ids != 0\n",
        "padded_input_ids, padded_position_ids, cache_attn_mask =  process_partial(input_text)\n",
        "\n",
        "print(f\"Raw Input IDs: {raw_input_ids.shape}\")\n",
        "print(f\"Attention Mask: {attn_mask.shape}\")\n",
        "print(f\"Padded Input IDs shape: {padded_input_ids.shape}\")\n",
        "print(f\"Position IDs shape: {padded_position_ids.shape}\")\n",
        "print(f\"Cache attention mask shape: {cache_attn_mask.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0qAphCP_qYVX",
      "metadata": {
        "id": "0qAphCP_qYVX"
      },
      "source": [
        "## Inference: Prefill + Auto-Regressive Generation\n",
        "\n",
        "## Prefill\n",
        "#\n",
        "The `prefill_partial` function is used to prefill the model with the input tokens. It takes the padded input IDs, positions, and attention mask as input and returns the logits and updated cache.\n",
        "#\n",
        "Note: The cache object is updated in-place by prefill_partial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XkXV_xrYqXKX",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XkXV_xrYqXKX",
        "outputId": "ade4e9f2-3d6c-4b0e-e0b9-0b928d628764"
      },
      "outputs": [],
      "source": [
        "logits, cache = prefill_partial(\n",
        "      padded_input_ids,\n",
        "      positions=padded_position_ids,\n",
        "      attn_mask=cache_attn_mask,\n",
        ")\n",
        "\n",
        "print(f\"Logits shape: {logits.shape}\")\n",
        "print(f\"Cache shape: {cache.shape}\" if hasattr(cache, \"shape\") else f\"Cache object: {type(cache)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MHuvW9c8qkls",
      "metadata": {
        "id": "MHuvW9c8qkls"
      },
      "source": [
        "\n",
        "## Generation\n",
        "\n",
        "Setup the scan function with the model, cache, and other parameters using `setup_scan_fn`.\n",
        "\n",
        "The `scan_generate_step` function is used to generate tokens in a loop. It takes the model, cache, and other parameters as input and returns the generated tokens and updated cache.\n",
        "\n",
        "Note: Using different inputs will not trigger recompilation as long as they fit within the padded window size. Generation will still run at  post-warmup speed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IYm00jBnqsvS",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYm00jBnqsvS",
        "outputId": "f2f6ef1c-3bbf-4416-f1fa-7bef0b645ee6"
      },
      "outputs": [],
      "source": [
        "all_gen, current_index, current_pos, carry = setup_scan_fn(\n",
        "    padded_input_ids,\n",
        "    padded_position_ids,\n",
        "    greedy_sample(logits,positions=padded_position_ids ),\n",
        "    prefill_cache=cache,\n",
        "    cache_length=config.cache_length,\n",
        ")\n",
        "\n",
        "# The carry tuple now includes the updated cache from prefill\n",
        "carry, _ = jax.lax.scan(generate_partial, carry, xs=None, length=2048)\n",
        "\n",
        "# Unpack final state after scan completes\n",
        "generated_tokens, final_cache = carry[0], carry[-1]\n",
        "print(f\"Generated tokens shape: {generated_tokens.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HkL0nWMsq7V4",
      "metadata": {
        "id": "HkL0nWMsq7V4"
      },
      "source": [
        "## Decode and Format Output\n",
        "\n",
        "\n",
        "Decode the generated tokens back to text using the tokenizer. This is generally the *slowest* step in the entire inference loop.\n",
        "Note the raw model is"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qkml4CTFq6Sb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkml4CTFq6Sb",
        "outputId": "ea550814-f5a7-4271-984e-b4e45567afde"
      },
      "outputs": [],
      "source": [
        "# Decode generated tokens\n",
        "formatted_output = decode_tokens(generated_tokens, tokenizer, skip_special_tokens=True)\n",
        "\n",
        "for i, output in enumerate(formatted_output):\n",
        "    print(f\"Output {i + 1}:\\n{output}\")\n",
        "    print(\"-\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Udepi04-sAzs",
      "metadata": {
        "id": "Udepi04-sAzs"
      },
      "source": [
        "## JIT Compiled Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VW7HngjMr_eo",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VW7HngjMr_eo",
        "outputId": "a889b691-b4f2-41f8-fa57-aeed18cc2a52"
      },
      "outputs": [],
      "source": [
        "input_text = [\n",
        "    \"The most beautiful thing in the world is\",\n",
        "    \"Do you know what MLA (multi head latent attention) is?. Recall that it is a recent innovation in LLM architecture, \\\n",
        "        from DeepSeek labs. If you do, explain what it is. Make sure to focus on implementation details, please!\",\n",
        "     \"Whats's your favorite Matsuo Basho poem? Translate it into Japanese.\",\n",
        "     \"Tell me a joke that is not too funny, but still kind of funny. Something that would make a 5 year old laugh. Or Larry David from Curb Your Enthusiasm fame.\",\n",
        "]\n",
        "\n",
        "input_text = [format_prompt(text) for text in input_text]  # Format for Gemma 3 dialogue\n",
        "\n",
        "padded_input_ids, padded_position_ids, cache_attn_mask =  process_partial(input_text)\n",
        "\n",
        "logits, cache = prefill_partial(\n",
        "    padded_input_ids,\n",
        "    positions=padded_position_ids,\n",
        "    attn_mask=cache_attn_mask,\n",
        ")\n",
        "\n",
        "all_gen, current_index, current_pos, carry = setup_scan_fn(\n",
        "    padded_input_ids,\n",
        "    padded_position_ids,\n",
        "    greedy_sample(logits,positions=padded_position_ids ),\n",
        "    prefill_cache=cache,\n",
        "    cache_length=config.cache_length,\n",
        ")\n",
        "\n",
        "carry, _ = jax.lax.scan(generate_partial, carry, xs=None, length=2048)\n",
        "generated_tokens, final_cache = carry[0], carry[-1]\n",
        "\n",
        "formatted_output = decode_tokens(generated_tokens, tokenizer, skip_special_tokens=True)\n",
        "for i, output in enumerate(formatted_output):\n",
        "    print(f\"Output {i + 1}:\\n{output}\")\n",
        "    print(\"-\" * 80)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
